name: Nightly Comprehensive Testing

on:
  schedule:
    # Run every night at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Scope of nightly testing'
        required: false
        default: 'comprehensive' 
        type: choice
        options:
        - comprehensive
        - extended
        - minimal
      include_stress_tests:
        description: 'Include stress testing'
        required: false
        default: true
        type: boolean
      notification_channel:
        description: 'Notification channel for results'
        required: false
        default: 'email'
        type: choice
        options:
        - email
        - slack
        - teams
        - none

env:
  PYTHONPATH: ${{ github.workspace }}/src
  NEURAL_ARCH_LOG_LEVEL: DEBUG  # More verbose for nightly runs
  NEURAL_ARCH_RANDOM_SEED: 42
  # Extended test configurations for nightly runs
  NIGHTLY_TEST_ITERATIONS: 1000
  STRESS_TEST_DURATION: 3600  # 1 hour stress tests
  MEMORY_LEAK_ITERATIONS: 10000

permissions:
  contents: read
  actions: write
  checks: write
  issues: write
  security-events: write

jobs:
  # ============================================================================
  # NIGHTLY CONFIGURATION - Setup and validation for nightly runs
  # ============================================================================
  
  nightly-setup:
    name: "ğŸŒ™ Nightly Testing Setup"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      test-scope: ${{ steps.config.outputs.test-scope }}
      matrix-config: ${{ steps.config.outputs.matrix-config }}
      stress-tests: ${{ steps.config.outputs.stress-tests }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Configure nightly testing
        id: config
        run: |
          # Determine test scope
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TEST_SCOPE="${{ github.event.inputs.test_scope }}"
            STRESS_TESTS="${{ github.event.inputs.include_stress_tests }}"
          else
            TEST_SCOPE="comprehensive"
            STRESS_TESTS="true"
          fi
          
          echo "test-scope=$TEST_SCOPE" >> $GITHUB_OUTPUT
          echo "stress-tests=$STRESS_TESTS" >> $GITHUB_OUTPUT
          
          # Configure test matrix based on scope
          case $TEST_SCOPE in
            "comprehensive")
              MATRIX_CONFIG='{"os": ["ubuntu-latest", "windows-latest", "macos-latest"], "python": ["3.8", "3.9", "3.10", "3.11", "3.12"], "backend": ["numpy", "jit"]}'
              ;;
            "extended")
              MATRIX_CONFIG='{"os": ["ubuntu-latest", "macos-latest"], "python": ["3.9", "3.11", "3.12"], "backend": ["numpy"]}'
              ;;
            "minimal")
              MATRIX_CONFIG='{"os": ["ubuntu-latest"], "python": ["3.11"], "backend": ["numpy"]}'
              ;;
          esac
          
          echo "matrix-config=$MATRIX_CONFIG" >> $GITHUB_OUTPUT
          
          echo "ğŸŒ™ Nightly Testing Configuration:"
          echo "â€¢ Scope: $TEST_SCOPE"
          echo "â€¢ Stress Tests: $STRESS_TESTS"
          echo "â€¢ Matrix: $MATRIX_CONFIG"
          
      - name: System health check
        run: |
          echo "ğŸ” Performing system health check..."
          
          # Check repository status
          git status --porcelain
          
          # Check disk space
          df -h
          
          # Check available memory
          free -h || echo "Memory info not available"
          
          # Check Python installation
          python3 --version
          
          echo "âœ… System health check completed"

  # ============================================================================
  # EXTENDED COMPATIBILITY TESTING - Comprehensive cross-platform validation
  # ============================================================================
  
  extended-compatibility:
    name: "ğŸ”§ Extended Compatibility (${{ matrix.os }}, Python ${{ matrix.python }}, ${{ matrix.backend }})"
    needs: nightly-setup
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        os: ${{ fromJson(needs.nightly-setup.outputs.matrix-config).os }}
        python: ${{ fromJson(needs.nightly-setup.outputs.matrix-config).python }}
        backend: ${{ fromJson(needs.nightly-setup.outputs.matrix-config).backend }}
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ matrix.python }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
          
      - name: Extended test suite execution
        run: |
          echo "ğŸ§ª Running extended test suite..."
          
          # Set backend for testing
          export NEURAL_ARCH_BACKEND=${{ matrix.backend }}
          
          # Run comprehensive test suite with extended timeout
          pytest tests/ -v --tb=long --strict-markers \
            --maxfail=20 --durations=50 \
            --timeout=300 --timeout-method=thread \
            --cov=neural_arch --cov-report=xml \
            --cov-fail-under=90 \
            --junit-xml=test-results-${{ matrix.os }}-${{ matrix.python }}-${{ matrix.backend }}.xml
            
      - name: Long-running integration tests
        run: |
          echo "ğŸ”„ Running long-running integration tests..."
          
          pytest tests/ -v --tb=short --strict-markers \
            -m "integration or slow" \
            --maxfail=10 --durations=30 \
            --timeout=600
            
      - name: Mathematical accuracy validation
        run: |
          echo "ğŸ”¢ Running mathematical accuracy validation..."
          
          python run_all_benchmarks.py > nightly-accuracy-${{ matrix.os }}-${{ matrix.python }}-${{ matrix.backend }}.log 2>&1 || true
          
          # Verify mathematical accuracy results exist
          if [ -f mathematical_accuracy_results.json ]; then
            echo "âœ… Mathematical accuracy results generated"
            python -c "
            import json
            with open('mathematical_accuracy_results.json', 'r') as f:
                results = json.load(f)
            
            passed = sum(1 for test in results.values() if test.get('status') == 'passed')
            total = len(results)
            accuracy = (passed / total) * 100 if total > 0 else 0
            
            print(f'Mathematical accuracy: {accuracy:.2f}% ({passed}/{total} tests passed)')
            
            if accuracy < 95.0:
                print('âš ï¸ Mathematical accuracy below 95%')
                exit(1)
            else:
                print('âœ… Mathematical accuracy meets requirements')
            "
          fi
          
      - name: Upload extended test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extended-test-results-${{ matrix.os }}-${{ matrix.python }}-${{ matrix.backend }}
          path: |
            test-results-*.xml
            coverage.xml
            nightly-accuracy-*.log
            mathematical_accuracy_results.json
          retention-days: 7

  # ============================================================================
  # STRESS TESTING - High-load and endurance testing
  # ============================================================================
  
  stress-testing:
    name: "ğŸ’ª Stress Testing"
    needs: nightly-setup
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours for stress tests
    if: needs.nightly-setup.outputs.stress-tests == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dependencies with monitoring tools
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
          pip install psutil memory-profiler
          
      - name: Memory leak detection
        run: |
          echo "ğŸ” Running memory leak detection..."
          
          python -c "
          import sys
          sys.path.insert(0, 'src')
          
          import gc
          import psutil
          import time
          from neural_arch.core import Tensor
          import numpy as np
          
          print('Starting memory leak detection...')
          process = psutil.Process()
          initial_memory = process.memory_info().rss / 1024 / 1024  # MB
          
          print(f'Initial memory usage: {initial_memory:.2f} MB')
          
          # Perform many tensor operations
          for i in range(${{ env.MEMORY_LEAK_ITERATIONS }}):
              if i % 1000 == 0:
                  current_memory = process.memory_info().rss / 1024 / 1024
                  print(f'Iteration {i}: {current_memory:.2f} MB')
                  
                  # Check for significant memory growth
                  if current_memory > initial_memory * 2:
                      print(f'âš ï¸ Potential memory leak detected at iteration {i}')
                      print(f'Memory grew from {initial_memory:.2f} MB to {current_memory:.2f} MB')
                      
              # Create and destroy tensors
              data = np.random.randn(100, 100)
              tensor = Tensor(data)
              result = tensor + tensor
              result = result * 2.0
              del tensor, result, data
              
              if i % 1000 == 0:
                  gc.collect()  # Force garbage collection
                  
          final_memory = process.memory_info().rss / 1024 / 1024
          print(f'Final memory usage: {final_memory:.2f} MB')
          print(f'Memory growth: {final_memory - initial_memory:.2f} MB')
          
          if final_memory > initial_memory * 1.5:
              print('âš ï¸ Significant memory growth detected')
              exit(1)
          else:
              print('âœ… Memory usage stable')
          " > stress-memory-leak.log 2>&1
          
      - name: High-load performance testing
        run: |
          echo "ğŸš€ Running high-load performance testing..."
          
          python -c "
          import sys
          sys.path.insert(0, 'src')
          
          import time
          import concurrent.futures
          from neural_arch.models.language import GPT2
          from neural_arch.core import Tensor
          import numpy as np
          
          print('Starting high-load performance testing...')
          
          def create_and_run_model(model_id):
              model = GPT2(vocab_size=1000, embed_dim=128, num_heads=4, num_layers=2)
              input_data = Tensor(np.random.randint(0, 1000, (1, 32)))
              
              start_time = time.time()
              for _ in range(100):
                  output = model(input_data)
              end_time = time.time()
              
              return {
                  'model_id': model_id,
                  'duration': end_time - start_time,
                  'output_shape': output.shape
              }
          
          # Run multiple models concurrently
          start_time = time.time()
          with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
              futures = [executor.submit(create_and_run_model, i) for i in range(8)]
              results = [future.result() for future in concurrent.futures.as_completed(futures)]
          end_time = time.time()
          
          total_time = end_time - start_time
          avg_time = sum(r['duration'] for r in results) / len(results)
          
          print(f'High-load test completed in {total_time:.2f}s')
          print(f'Average model execution time: {avg_time:.2f}s')
          print(f'Concurrent models: {len(results)}')
          
          if avg_time > 30.0:  # Should complete in reasonable time
              print('âš ï¸ Performance degradation detected under high load')
              exit(1)
          else:
              print('âœ… High-load performance acceptable')
          " > stress-performance.log 2>&1
          
      - name: Endurance testing
        run: |
          echo "â±ï¸ Running endurance testing..."
          
          timeout ${{ env.STRESS_TEST_DURATION }} python -c "
          import sys
          sys.path.insert(0, 'src')
          
          import time
          import random
          from neural_arch.models.language import GPT2
          from neural_arch.models.vision import VisionTransformer
          from neural_arch.core import Tensor
          import numpy as np
          
          print('Starting endurance testing...')
          start_time = time.time()
          iteration = 0
          
          models = {
              'gpt2': GPT2(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=2),
              'vit': VisionTransformer(img_size=32, patch_size=4, embed_dim=64, num_heads=4, num_layers=2)
          }
          
          try:
              while True:
                  iteration += 1
                  current_time = time.time()
                  elapsed = current_time - start_time
                  
                  if iteration % 100 == 0:
                      print(f'Endurance test iteration {iteration}, elapsed: {elapsed:.1f}s')
                  
                  # Randomly select and run a model
                  model_name = random.choice(list(models.keys()))
                  model = models[model_name]
                  
                  if model_name == 'gpt2':
                      input_data = Tensor(np.random.randint(0, 1000, (1, 16)))
                  else:  # vit
                      input_data = Tensor(np.random.randn(1, 3, 32, 32))
                  
                  output = model(input_data)
                  
                  # Simulate some processing delay
                  time.sleep(0.01)
                  
          except KeyboardInterrupt:
              pass
          
          final_time = time.time()
          total_elapsed = final_time - start_time
          
          print(f'Endurance test completed after {total_elapsed:.1f}s')
          print(f'Total iterations: {iteration}')
          print(f'Average iteration time: {total_elapsed/iteration:.4f}s')
          print('âœ… Endurance test completed successfully')
          " > stress-endurance.log 2>&1 || echo "Endurance test completed (timeout expected)"
          
      - name: Upload stress test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stress-test-results
          path: |
            stress-*.log
          retention-days: 7

  # ============================================================================
  # SECURITY DEEP SCAN - Comprehensive security validation
  # ============================================================================
  
  security-deep-scan:
    name: "ğŸ”’ Security Deep Scan"
    needs: nightly-setup
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install comprehensive security tools
        run: |
          python -m pip install --upgrade pip wheel
          pip install bandit[toml] safety semgrep pip-audit cyclonedx-bom
          pip install -e .
          
      - name: Deep security scan with Bandit
        run: |
          echo "ğŸ”’ Running comprehensive Bandit security scan..."
          
          # Comprehensive scan with all checks
          bandit -r src/ -f json -o bandit-deep-scan.json --severity-level all --confidence-level all || true
          bandit -r src/ --severity-level all --confidence-level all
          
      - name: Advanced dependency scanning
        run: |
          echo "ğŸ” Running advanced dependency vulnerability scanning..."
          
          # Multiple tools for comprehensive coverage
          safety check --full-report --json --output safety-full-report.json || true
          safety check --full-report
          
          pip-audit --format=json --output=pip-audit-full.json --desc || true
          pip-audit --desc
          
      - name: Generate Software Bill of Materials (SBOM)
        run: |
          echo "ğŸ“‹ Generating Software Bill of Materials..."
          
          # Create SBOM for supply chain security
          cyclonedx-py --output-format json --output-file sbom.json . || true
          
      - name: Code pattern security analysis
        run: |
          echo "ğŸ” Running code pattern security analysis..."
          
          # Look for common security anti-patterns
          python -c "
          import os
          import re
          
          security_patterns = {
              'hardcoded_secrets': r'(password|secret|key|token)\s*=\s*[\"'][^\"']{8,}[\"']',
              'sql_injection': r'(execute|query|cursor)\s*\(\s*[\"'].*%s.*[\"']',
              'command_injection': r'(os\.system|subprocess|shell=True)',
              'eval_usage': r'eval\s*\(',
              'unsafe_pickle': r'pickle\.loads?\s*\(',
          }
          
          findings = []
          
          for root, dirs, files in os.walk('src'):
              for file in files:
                  if file.endswith('.py'):
                      filepath = os.path.join(root, file)
                      try:
                          with open(filepath, 'r', encoding='utf-8') as f:
                              content = f.read()
                              
                          for pattern_name, pattern in security_patterns.items():
                              matches = re.finditer(pattern, content, re.IGNORECASE)
                              for match in matches:
                                  findings.append({
                                      'file': filepath,
                                      'pattern': pattern_name,
                                      'line': content[:match.start()].count('\\n') + 1,
                                      'match': match.group()
                                  })
                      except Exception as e:
                          print(f'Error scanning {filepath}: {e}')
          
          print(f'Security pattern analysis: {len(findings)} findings')
          for finding in findings:
              print(f\"  {finding['file']}:{finding['line']} - {finding['pattern']}\")
          
          if findings:
              print('âš ï¸ Security patterns detected - manual review required')
          else:
              print('âœ… No concerning security patterns found')
          " > security-pattern-analysis.log 2>&1
          
      - name: Upload security scan results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-deep-scan-results
          path: |
            bandit-deep-scan.json
            safety-full-report.json
            pip-audit-full.json
            sbom.json
            security-pattern-analysis.log
          retention-days: 30

  # ============================================================================
  # PERFORMANCE REGRESSION ANALYSIS - Extended performance validation
  # ============================================================================
  
  performance-regression:
    name: "ğŸ“ˆ Performance Regression Analysis"
    needs: nightly-setup
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 30  # Get more history for trend analysis
          
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel
          pip install -e ".[dev]"
          pip install pytest-benchmark pandas matplotlib seaborn
          
      - name: Extended performance benchmarking
        run: |
          echo "ğŸ“Š Running extended performance benchmarks..."
          
          # Run comprehensive benchmarks with more iterations
          pytest tests/ -v --tb=short --strict-markers \
            -m "benchmark" \
            --benchmark-json=nightly-performance-results.json \
            --benchmark-min-rounds=20 --benchmark-warmup=5 \
            --benchmark-sort=mean --benchmark-group-by=group
            
      - name: Historical performance comparison
        run: |
          echo "ğŸ“ˆ Analyzing performance trends..."
          
          python -c "
          import json
          import subprocess
          from datetime import datetime, timedelta
          
          # Get recent commits for trend analysis
          result = subprocess.run(['git', 'log', '--oneline', '-10'], 
                                capture_output=True, text=True)
          commits = result.stdout.strip().split('\\n')
          
          print('Recent commits for performance analysis:')
          for i, commit in enumerate(commits[:5]):
              print(f'  {i+1}. {commit}')
          
          # Load current benchmark results
          try:
              with open('nightly-performance-results.json', 'r') as f:
                  current_results = json.load(f)
                  
              benchmarks = current_results.get('benchmarks', [])
              print(f'\\nCurrent benchmark results: {len(benchmarks)} tests')
              
              # Show top 10 slowest benchmarks
              sorted_benchmarks = sorted(benchmarks, 
                                       key=lambda x: x.get('stats', {}).get('mean', 0), 
                                       reverse=True)
              
              print('\\nTop 10 slowest operations:')
              for i, bench in enumerate(sorted_benchmarks[:10]):
                  name = bench.get('name', 'Unknown')
                  mean_time = bench.get('stats', {}).get('mean', 0)
                  print(f'  {i+1}. {name}: {mean_time:.6f}s')
                  
          except Exception as e:
              print(f'Error analyzing results: {e}')
          " > performance-trend-analysis.log 2>&1
          
      - name: Generate performance report
        run: |
          echo "ğŸ“‹ Generating comprehensive performance report..."
          
          cat > nightly-performance-report.md << 'EOF'
          # Nightly Performance Analysis Report
          
          ## Executive Summary
          Comprehensive performance analysis conducted as part of nightly testing cycle.
          
          ## Test Configuration
          - **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          - **Branch**: ${{ github.ref_name }}
          - **Commit**: ${{ github.sha }}
          - **Environment**: Ubuntu Latest, Python 3.11
          - **Benchmark Rounds**: 20 (with 5 warmup rounds)
          
          ## Key Findings
          EOF
          
          if [ -f nightly-performance-results.json ]; then
            python -c "
            import json
            with open('nightly-performance-results.json', 'r') as f:
                data = json.load(f)
                
            benchmarks = data.get('benchmarks', [])
            if benchmarks:
                mean_times = [b.get('stats', {}).get('mean', 0) for b in benchmarks]
                avg_time = sum(mean_times) / len(mean_times)
                max_time = max(mean_times)
                min_time = min(mean_times)
                
                print(f'- **Total Benchmarks**: {len(benchmarks)}')
                print(f'- **Average Execution Time**: {avg_time:.6f}s')
                print(f'- **Fastest Operation**: {min_time:.6f}s')
                print(f'- **Slowest Operation**: {max_time:.6f}s')
            " >> nightly-performance-report.md
          fi
          
          echo '' >> nightly-performance-report.md
          echo '## Trend Analysis' >> nightly-performance-report.md
          echo 'See performance-trend-analysis.log for detailed historical comparison.' >> nightly-performance-report.md
          echo '' >> nightly-performance-report.md
          echo '## Recommendations' >> nightly-performance-report.md
          echo '1. Monitor operations taking >1s for optimization opportunities' >> nightly-performance-report.md
          echo '2. Track performance trends over multiple nightly runs' >> nightly-performance-report.md
          echo '3. Investigate any significant regressions immediately' >> nightly-performance-report.md
          
      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: nightly-performance-analysis
          path: |
            nightly-performance-results.json
            performance-trend-analysis.log
            nightly-performance-report.md
          retention-days: 30

  # ============================================================================
  # COMPREHENSIVE REPORTING - Aggregate all nightly test results
  # ============================================================================
  
  nightly-report:
    name: "ğŸ“Š Nightly Testing Report"
    needs: [
      nightly-setup,
      extended-compatibility,
      stress-testing, 
      security-deep-scan,
      performance-regression
    ]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download all nightly results
        uses: actions/download-artifact@v4
        with:
          path: nightly-results/
          
      - name: Generate comprehensive nightly report
        run: |
          echo "ğŸ“Š Generating comprehensive nightly testing report..."
          
          cat > nightly-comprehensive-report.md << 'EOF'
          # Neural Architecture Framework - Nightly Testing Report
          
          ## Overview
          Comprehensive nightly testing results for enterprise-grade validation.
          
          ## Execution Summary
          - **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          - **Trigger**: ${{ github.event_name }}
          - **Branch**: ${{ github.ref_name }}
          - **Commit**: ${{ github.sha }}
          - **Test Scope**: ${{ needs.nightly-setup.outputs.test-scope }}
          
          ## Test Coverage
          EOF
          
          # Count test results
          COMPATIBILITY_RESULTS=$(find nightly-results -name "*extended-test-results*" | wc -l)
          STRESS_RESULTS=$(find nightly-results -name "*stress-test-results*" | wc -l) 
          SECURITY_RESULTS=$(find nightly-results -name "*security-deep-scan*" | wc -l)
          PERFORMANCE_RESULTS=$(find nightly-results -name "*performance-analysis*" | wc -l)
          
          cat >> nightly-comprehensive-report.md << EOF
          - **Compatibility Tests**: $COMPATIBILITY_RESULTS configurations
          - **Stress Tests**: $STRESS_RESULTS suites
          - **Security Scans**: $SECURITY_RESULTS comprehensive scans
          - **Performance Analysis**: $PERFORMANCE_RESULTS benchmark suites
          
          ## Quality Gates Status
          EOF
          
          # Determine overall status based on job results
          NEEDS_JSON='${{ toJson(needs) }}'
          
          echo "$NEEDS_JSON" | python3 -c "
          import json
          import sys
          
          needs = json.load(sys.stdin)
          
          statuses = {
              'extended-compatibility': needs.get('extended-compatibility', {}).get('result', 'unknown'),
              'stress-testing': needs.get('stress-testing', {}).get('result', 'unknown'),
              'security-deep-scan': needs.get('security-deep-scan', {}).get('result', 'unknown'),
              'performance-regression': needs.get('performance-regression', {}).get('result', 'unknown')
          }
          
          for job, status in statuses.items():
              icon = 'âœ…' if status == 'success' else 'âŒ' if status == 'failure' else 'â­ï¸'
              print(f'- **{job.replace(\"-\", \" \").title()}**: {icon} {status}')
          
          # Overall status
          failed_jobs = [job for job, status in statuses.items() if status == 'failure']
          if failed_jobs:
              print(f'\\nâš ï¸ **Overall Status**: FAILED ({len(failed_jobs)} jobs failed)')
              print('\\n### Failed Jobs:')
              for job in failed_jobs:
                  print(f'- {job.replace(\"-\", \" \").title()}')
          else:
              print('\\nâœ… **Overall Status**: PASSED')
          " >> nightly-comprehensive-report.md
          
          cat >> nightly-comprehensive-report.md << 'EOF'
          
          ## Key Metrics
          - **Test Duration**: Complete nightly cycle
          - **Coverage Target**: 95%+ across all components  
          - **Performance Threshold**: 10% regression tolerance
          - **Security Scanning**: Comprehensive SAST + DAST
          
          ## Artifacts Generated
          - Extended compatibility test results
          - Stress testing logs and metrics
          - Security scan reports and SBOM
          - Performance benchmarks and trend analysis
          
          ## Next Steps
          1. Review any failed quality gates immediately
          2. Investigate performance regressions
          3. Address security findings if any
          4. Update dashboards with latest metrics
          
          ## Contact Information
          - **CI/CD Issues**: DevOps Team
          - **Test Failures**: QA Team  
          - **Security Concerns**: Security Team
          - **Performance Issues**: Performance Engineering Team
          
          ---
          *Generated by Neural Architecture Framework Nightly Testing Pipeline*
          EOF
          
      - name: Create issue for failures
        if: contains(needs.*.result, 'failure')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let report = '# ğŸŒ™ Nightly Testing Failure Report\n\n';
            report += `**Date**: ${new Date().toISOString()}\n`;
            report += `**Branch**: ${{ github.ref_name }}\n`;
            report += `**Commit**: ${{ github.sha }}\n`;
            report += `**Workflow**: ${{ github.run_id }}\n\n`;
            
            const needs = ${{ toJson(needs) }};
            const failedJobs = [];
            
            for (const [jobName, jobResult] of Object.entries(needs)) {
              if (jobResult.result === 'failure') {
                failedJobs.push(jobName);
              }
            }
            
            if (failedJobs.length > 0) {
              report += '## âŒ Failed Jobs\n\n';
              failedJobs.forEach(job => {
                report += `- **${job.replace(/-/g, ' ').replace(/\b\w/g, l => l.toUpperCase())}**\n`;
              });
              report += '\n';
            }
            
            report += '## ğŸ” Investigation Steps\n\n';
            report += '1. Review failed job logs in the workflow run\n';
            report += '2. Check for environmental issues or flaky tests\n'; 
            report += '3. Verify if issues reproduce locally\n';
            report += '4. Create targeted fixes or test improvements\n\n';
            
            report += '## ğŸ“Š Full Report\n\n';
            report += 'See workflow artifacts for detailed results and logs.\n';
            
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ğŸŒ™ Nightly Testing Failures - ${new Date().toISOString().split('T')[0]}`,
              body: report,
              labels: ['ci/cd', 'nightly-testing', 'bug']
            });
            
            console.log(`Created issue: ${issue.data.html_url}`);
            
      - name: Nightly testing summary
        run: |
          echo "ğŸŒ™ =============================="
          echo "ğŸŒ™ NIGHTLY TESTING COMPLETE"
          echo "ğŸŒ™ =============================="
          echo ""
          echo "ğŸ“Š Test Summary:"
          echo "â€¢ Extended Compatibility: $COMPATIBILITY_RESULTS configurations"  
          echo "â€¢ Stress Testing: Completed"
          echo "â€¢ Security Deep Scan: Comprehensive"
          echo "â€¢ Performance Analysis: Full benchmark suite"
          echo ""
          echo "ğŸ” Review Results:"
          echo "â€¢ Download artifacts for detailed analysis"
          echo "â€¢ Check performance trends and regressions" 
          echo "â€¢ Review security scan findings"
          echo "â€¢ Validate mathematical accuracy results"
          echo ""
          echo "ğŸ“ˆ Next Nightly Run: Tomorrow at 2 AM UTC"
          
      - name: Upload comprehensive nightly report
        uses: actions/upload-artifact@v4
        with:
          name: nightly-comprehensive-report
          path: nightly-comprehensive-report.md
          retention-days: 90