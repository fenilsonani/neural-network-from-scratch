# ğŸ§  Neural Architecture - Complete Implementation From Scratch

[![Tests](https://github.com/fenilsonani/neural-network-from-scratch/actions/workflows/tests.yml/badge.svg)](https://github.com/fenilsonani/neural-network-from-scratch/actions/workflows/tests.yml)
[![Documentation](https://readthedocs.org/projects/neural-arch/badge/?version=latest)](https://neural-arch.readthedocs.io/en/latest/?badge=latest)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **production-ready neural network implementation** built from scratch using only NumPy. Complete with transformer architecture, comprehensive testing, performance benchmarks, and a working translation application.

## ğŸš€ What This Is

**The most comprehensive neural network implementation from scratch**, featuring:

- ğŸ¯ **Custom tensor system** with automatic differentiation
- ğŸ§± **Complete neural layers** (Linear, Embedding, LayerNorm, Multi-Head Attention, Dropout)
- âš¡ **Advanced optimizers** (Adam with gradient clipping and proper parameter handling)
- ğŸ¤– **Full transformer architecture** (encoder-decoder, attention, positional encoding)
- ğŸŒ **Working translation application** (English-Spanish using Tatoeba dataset)
- ğŸ“Š **Extensive test suite** (182 comprehensive tests - all passing!)
- ğŸƒâ€â™‚ï¸ **Performance benchmarks** and regression testing
- ğŸ›¡ï¸ **Production-ready** with numerical stability guarantees

## ğŸ¯ What It Can Do

### **Translation & Language Tasks**
- ğŸŒ **Machine Translation** - Working English-Spanish translator
- ğŸ“ **Text Generation** with transformer architecture
- ğŸ”„ **Sequence-to-sequence** tasks with attention mechanisms
- ğŸ“š **Language modeling** with state-of-the-art architecture

### **Core Neural Network Features**
- ğŸ—ï¸ **Transformer Blocks** - Multi-head attention, layer normalization
- ğŸ­ **Encoder-Decoder Architecture** - Full seq2seq capabilities
- ğŸ§® **Automatic Differentiation** - Complete backpropagation
- ğŸ“ˆ **Advanced Training** - Gradient clipping, learning rate scheduling

### **Research & Education**
- ğŸ“ **Learning neural networks** from first principles
- ğŸ”¬ **Research experiments** with custom architectures
- ğŸ“Š **Performance analysis** and optimization studies
- ğŸ› ï¸ **Algorithm development** without framework constraints

## ğŸ“ Project Structure

```
nural-arch/
â”œâ”€â”€ src/neural_arch/
â”‚   â”œâ”€â”€ core/                        # Core tensor and module system
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Core exports
â”‚   â”‚   â”œâ”€â”€ base.py                 # Module base class with parameters
â”‚   â”‚   â””â”€â”€ tensor.py               # Tensor with autograd
â”‚   â”œâ”€â”€ nn/                         # Neural network layers
â”‚   â”‚   â”œâ”€â”€ __init__.py            # NN exports
â”‚   â”‚   â”œâ”€â”€ linear.py              # Linear layer
â”‚   â”‚   â”œâ”€â”€ embedding.py           # Embedding layer (fixed for Tensor input)
â”‚   â”‚   â”œâ”€â”€ normalization.py       # LayerNorm implementation
â”‚   â”‚   â”œâ”€â”€ dropout.py             # Dropout layer
â”‚   â”‚   â”œâ”€â”€ attention.py           # Multi-head attention
â”‚   â”‚   â””â”€â”€ transformer.py         # Transformer blocks
â”‚   â”œâ”€â”€ functional/                 # Functional operations
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Functional exports
â”‚   â”‚   â”œâ”€â”€ activation.py         # ReLU, Softmax, etc.
â”‚   â”‚   â”œâ”€â”€ loss.py              # Cross-entropy loss
â”‚   â”‚   â””â”€â”€ utils.py             # Helper functions
â”‚   â””â”€â”€ optim/                     # Optimizers
â”‚       â”œâ”€â”€ __init__.py           # Optimizer exports
â”‚       â””â”€â”€ adam.py               # Adam optimizer (fixed parameter handling)
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ translation/               # Translation application
â”‚       â”œâ”€â”€ model_v2.py           # Working transformer model
â”‚       â”œâ”€â”€ vocabulary.py         # Vocabulary management
â”‚       â”œâ”€â”€ train_conversational.py # Training script
â”‚       â”œâ”€â”€ translate.py          # Interactive translator
â”‚       â”œâ”€â”€ process_spa_file.py   # Process Tatoeba data
â”‚       â””â”€â”€ data/                 # Training data (gitignored)
â”œâ”€â”€ tests/                        # Comprehensive test suite (182 tests)
â”‚   â”œâ”€â”€ test_tensor.py           # Core tensor operations
â”‚   â”œâ”€â”€ test_layers.py           # Neural network layers
â”‚   â”œâ”€â”€ test_optimizer.py        # Optimizer tests
â”‚   â”œâ”€â”€ test_training.py         # Training pipeline
â”‚   â”œâ”€â”€ test_transformer.py      # NEW: Transformer components
â”‚   â”œâ”€â”€ test_translation_model.py # NEW: Translation model
â”‚   â””â”€â”€ test_adam_optimizer.py   # NEW: Adam improvements
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ sphinx/                  # Sphinx documentation
â”‚   â”œâ”€â”€ API_REFERENCE.md        # Complete API reference
â”‚   â””â”€â”€ CHANGELOG.md            # Version history
â””â”€â”€ README.md                   # This file
```

## âš¡ Quick Start

### 1. **Install Dependencies**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install numpy pytest
```

### 2. **Run Comprehensive Tests**
```bash
pytest -v
# ğŸ‰ 182 tests, 0 failed, 1 warning
```

### 3. **Try the Translation App**
```bash
cd examples/translation

# Download and process Tatoeba dataset
python process_spa_file.py  # Requires spa.txt from Tatoeba

# Train the model
python train_conversational.py

# Use the translator
python translate.py
```

## ğŸ§  Core Architecture

### **Advanced Tensor System**
```python
from neural_arch.core import Tensor, Parameter
from neural_arch.functional import matmul, softmax

# Automatic differentiation with gradient tracking
a = Tensor([[1, 2, 3]], requires_grad=True)
b = Tensor([[4], [5], [6]], requires_grad=True)
c = matmul(a, b)  # Matrix multiplication with gradients
c.backward()      # Automatic backpropagation
```

### **Transformer Architecture**
```python
from neural_arch.nn import TransformerBlock, MultiHeadAttention

# State-of-the-art transformer block
transformer = TransformerBlock(
    d_model=512,
    num_heads=8,
    d_ff=2048,
    dropout=0.1
)

# Multi-head attention with masking
attention = MultiHeadAttention(d_model=512, num_heads=8)
output = attention(query, key, value, mask=attention_mask)
```

### **Translation Model**
```python
from examples.translation.model_v2 import TranslationTransformer
from examples.translation.vocabulary import Vocabulary

# Complete translation model
model = TranslationTransformer(
    src_vocab_size=10000,
    tgt_vocab_size=10000,
    d_model=256,
    n_heads=8,
    n_layers=6
)

# Vocabulary management
src_vocab = Vocabulary("english")
tgt_vocab = Vocabulary("spanish")

# Training
optimizer = Adam(model.parameters(), lr=0.001)
```

## âœ¨ Key Features

### **ğŸ¯ Production Ready**
- âœ… **Comprehensive testing** - 182 tests covering every scenario
- âœ… **Parameter handling fixed** - Proper integration with optimizers
- âœ… **Gradient flow verified** - Complete backpropagation through transformers
- âœ… **Numerical stability** - Gradient clipping and proper initialization
- âœ… **Memory efficient** - Proper cleanup and parameter management

### **ğŸš€ New Features**
- âœ… **Transformer architecture** - Full encoder-decoder implementation
- âœ… **Multi-head attention** - With proper masking support
- âœ… **Layer normalization** - For training stability
- âœ… **Positional encoding** - Sinusoidal position embeddings
- âœ… **Translation application** - Working English-Spanish translator

### **ğŸ›¡ï¸ Robustness**
- âœ… **Fixed optimizer integration** - Parameters properly passed to Adam
- âœ… **Embedding layer fixed** - Handles both Tensor and numpy inputs
- âœ… **Gradient clipping** - Prevents exploding gradients
- âœ… **Proper masking** - Attention and padding masks
- âœ… **Loss calculation** - Correctly ignores padding tokens

## ğŸ§ª Testing Excellence

### **182 Comprehensive Tests**
```bash
ğŸ‰ EXTENSIVE TEST SUITE RESULTS:
=====================================
âœ… Core Tests: 60/60 passed
âœ… Advanced Tests: 17/17 passed  
âœ… Transformer Tests: 19/19 passed
âœ… Performance Tests: 11/11 passed
âœ… Edge Case Tests: 22/22 passed
âœ… NEW Transformer Components: 16/16 passed
âœ… NEW Translation Model: 16/16 passed
âœ… NEW Adam Optimizer: 13/13 passed
âœ… Stress Tests: 8/8 passed

ğŸ“Š Total: 182/182 tests passed (100%)
â±ï¸ Execution time: ~5.5 seconds
```

### **New Test Categories**
- ğŸ¤– **Transformer Components** - Attention, blocks, layer norm
- ğŸŒ **Translation Model** - Vocabulary, dataset, full pipeline
- âš¡ **Optimizer Improvements** - Parameter handling, convergence

## ğŸ“ˆ Recent Improvements

### **1. Fixed Parameter Access Bug**
```python
# Before: Parameters returned as strings
model.parameters()  # ['weight', 'bias'] âŒ

# After: Parameters returned correctly
model.parameters()  # [Parameter(...), Parameter(...)] âœ…
```

### **2. Gradient Flow Through Transformers**
- Connected gradients between loss and model output
- Proper backward pass through attention layers
- Gradient clipping for stability

### **3. Translation Application**
- Vocabulary management with special tokens
- Tatoeba dataset processing (120k+ pairs)
- Interactive translation interface
- Optimized training for CPU

## ğŸŒŸ Translation Application

### **Features**
- ğŸ“š **Tatoeba Dataset** - 120k+ conversational sentence pairs
- ğŸ”„ **Bidirectional** - Handles both encoding and decoding
- ğŸ¯ **Attention Visualization** - See what the model focuses on
- ğŸ’¬ **Interactive Mode** - Real-time translation

### **Usage**
```python
# Process dataset
python process_spa_file.py  # Creates train/val/test splits

# Train model
python train_conversational.py
# Epoch 1/100 - Loss: 6.2768
# Epoch 50/100 - Loss: 2.1453
# Translation Examples:
#   hello â†’ hola
#   how are you â†’ cÃ³mo estÃ¡s

# Interactive translation
python translate.py
# ğŸ‡¬ğŸ‡§ English: hello world
# ğŸ‡ªğŸ‡¸ Spanish: hola mundo
```

## ğŸ“š Documentation Updates

- ğŸ“„ **README.md** - Updated with all new features
- ğŸ§ª **Test Documentation** - Coverage of new components
- ğŸ“š **API Reference** - Transformer and translation APIs
- ğŸ“‹ **CHANGELOG.md** - Detailed version history

## ğŸš€ Getting Started

1. **Clone and setup**:
   ```bash
   git clone https://github.com/fenilsonani/neural-network-from-scratch.git
   cd neural-network-from-scratch
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Run all tests**:
   ```bash
   pytest -v
   ```

3. **Try translation**:
   ```bash
   cd examples/translation
   # Download spa.txt from Tatoeba first
   python process_spa_file.py
   python train_conversational.py
   ```

## ğŸ¤ Contributing

See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

MIT License - **Use it however you want!**

---

## ğŸ‰ Summary

**Production-ready neural network with transformer architecture and real-world application.**

- ğŸ§  **Complete implementation** from scratch
- ğŸ¤– **Transformer architecture** with attention mechanisms
- ğŸŒ **Working translator** with 120k+ training pairs
- ğŸ§ª **182 tests** all passing
- ğŸ“š **Comprehensive docs** and examples
- âš¡ **Optimized** for learning and research

**Ready for translation tasks, research, and education!** ğŸš€